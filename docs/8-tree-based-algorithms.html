<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>STA 578 - Statistical Computing Notes</title>
  <meta name="description" content="STA 578 - Statistical Computing Notes">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="STA 578 - Statistical Computing Notes" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="STA 578 - Statistical Computing Notes" />
  
  
  

<meta name="author" content="Derek Sonderegger">


<meta name="date" content="2017-11-07">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="7-beyond-linearity.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Computing</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="1-data-manipulation.html"><a href="1-data-manipulation.html"><i class="fa fa-check"></i><b>1</b> Data Manipulation</a><ul>
<li class="chapter" data-level="1.1" data-path="1-data-manipulation.html"><a href="1-data-manipulation.html#classic-r-functions-for-summarizing-rows-and-columns"><i class="fa fa-check"></i><b>1.1</b> Classic R functions for summarizing rows and columns</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-data-manipulation.html"><a href="1-data-manipulation.html#summary"><i class="fa fa-check"></i><b>1.1.1</b> <code>summary()</code></a></li>
<li class="chapter" data-level="1.1.2" data-path="1-data-manipulation.html"><a href="1-data-manipulation.html#apply"><i class="fa fa-check"></i><b>1.1.2</b> <code>apply()</code></a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-data-manipulation.html"><a href="1-data-manipulation.html#package-dplyr"><i class="fa fa-check"></i><b>1.2</b> Package <code>dplyr</code></a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-data-manipulation.html"><a href="1-data-manipulation.html#verbs"><i class="fa fa-check"></i><b>1.2.1</b> Verbs</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-data-manipulation.html"><a href="1-data-manipulation.html#split-apply-combine"><i class="fa fa-check"></i><b>1.2.2</b> Split, apply, combine</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-data-manipulation.html"><a href="1-data-manipulation.html#chaining-commands-together"><i class="fa fa-check"></i><b>1.2.3</b> Chaining commands together</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-data-manipulation.html"><a href="1-data-manipulation.html#reshaping-data"><i class="fa fa-check"></i><b>1.3</b> Reshaping data</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-data-manipulation.html"><a href="1-data-manipulation.html#tidyr"><i class="fa fa-check"></i><b>1.3.1</b> <code>tidyr</code></a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-data-manipulation.html"><a href="1-data-manipulation.html#storing-data-in-multiple-tables"><i class="fa fa-check"></i><b>1.4</b> Storing Data in Multiple Tables</a><ul>
<li class="chapter" data-level="1.4.1" data-path="1-data-manipulation.html"><a href="1-data-manipulation.html#table-joins"><i class="fa fa-check"></i><b>1.4.1</b> Table Joins</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="1-data-manipulation.html"><a href="1-data-manipulation.html#exercises"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-markov-chain-monte-carlo.html"><a href="2-markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>2</b> Markov Chain Monte Carlo</a><ul>
<li class="chapter" data-level="2.1" data-path="2-markov-chain-monte-carlo.html"><a href="2-markov-chain-monte-carlo.html#generating-usim-uniform01"><i class="fa fa-check"></i><b>2.1</b> Generating <span class="math inline">\(U\sim Uniform(0,1)\)</span></a></li>
<li class="chapter" data-level="2.2" data-path="2-markov-chain-monte-carlo.html"><a href="2-markov-chain-monte-carlo.html#inverse-cdf-method"><i class="fa fa-check"></i><b>2.2</b> Inverse CDF Method</a></li>
<li class="chapter" data-level="2.3" data-path="2-markov-chain-monte-carlo.html"><a href="2-markov-chain-monte-carlo.html#acceptreject-algorithm"><i class="fa fa-check"></i><b>2.3</b> Accept/Reject Algorithm</a></li>
<li class="chapter" data-level="2.4" data-path="2-markov-chain-monte-carlo.html"><a href="2-markov-chain-monte-carlo.html#mcmc-algorithm"><i class="fa fa-check"></i><b>2.4</b> MCMC algorithm</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-markov-chain-monte-carlo.html"><a href="2-markov-chain-monte-carlo.html#mixture-of-normals"><i class="fa fa-check"></i><b>2.4.1</b> Mixture of normals</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-markov-chain-monte-carlo.html"><a href="2-markov-chain-monte-carlo.html#common-problems"><i class="fa fa-check"></i><b>2.4.2</b> Common problems</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-markov-chain-monte-carlo.html"><a href="2-markov-chain-monte-carlo.html#assessing-chain-convergence"><i class="fa fa-check"></i><b>2.4.3</b> Assessing Chain Convergence</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-markov-chain-monte-carlo.html"><a href="2-markov-chain-monte-carlo.html#multi-variate-mcmc"><i class="fa fa-check"></i><b>2.5</b> Multi-variate MCMC</a></li>
<li class="chapter" data-level="2.6" data-path="2-markov-chain-monte-carlo.html"><a href="2-markov-chain-monte-carlo.html#hamiltonian-mcmc"><i class="fa fa-check"></i><b>2.6</b> Hamiltonian MCMC</a></li>
<li class="chapter" data-level="2.7" data-path="2-markov-chain-monte-carlo.html"><a href="2-markov-chain-monte-carlo.html#exercises-1"><i class="fa fa-check"></i><b>2.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-overview-of-statistical-learning.html"><a href="3-overview-of-statistical-learning.html"><i class="fa fa-check"></i><b>3</b> Overview of Statistical Learning</a><ul>
<li class="chapter" data-level="3.1" data-path="3-overview-of-statistical-learning.html"><a href="3-overview-of-statistical-learning.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>3.1</b> K-Nearest Neighbors</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-overview-of-statistical-learning.html"><a href="3-overview-of-statistical-learning.html#knn-for-classification"><i class="fa fa-check"></i><b>3.1.1</b> KNN for Classification</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-overview-of-statistical-learning.html"><a href="3-overview-of-statistical-learning.html#knn-for-regression"><i class="fa fa-check"></i><b>3.1.2</b> KNN for Regression</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-overview-of-statistical-learning.html"><a href="3-overview-of-statistical-learning.html#splitting-into-a-test-and-training-sets"><i class="fa fa-check"></i><b>3.2</b> Splitting into a test and training sets</a></li>
<li class="chapter" data-level="3.3" data-path="3-overview-of-statistical-learning.html"><a href="3-overview-of-statistical-learning.html#exercises-2"><i class="fa fa-check"></i><b>3.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-classification-with-lda-qda-and-knn.html"><a href="4-classification-with-lda-qda-and-knn.html"><i class="fa fa-check"></i><b>4</b> Classification with LDA, QDA, and KNN</a><ul>
<li class="chapter" data-level="4.1" data-path="4-classification-with-lda-qda-and-knn.html"><a href="4-classification-with-lda-qda-and-knn.html#logistic-regression"><i class="fa fa-check"></i><b>4.1</b> Logistic Regression</a></li>
<li class="chapter" data-level="4.2" data-path="4-classification-with-lda-qda-and-knn.html"><a href="4-classification-with-lda-qda-and-knn.html#roc-curves"><i class="fa fa-check"></i><b>4.2</b> ROC Curves</a></li>
<li class="chapter" data-level="4.3" data-path="4-classification-with-lda-qda-and-knn.html"><a href="4-classification-with-lda-qda-and-knn.html#linear-discriminent-analysis"><i class="fa fa-check"></i><b>4.3</b> Linear Discriminent Analysis</a></li>
<li class="chapter" data-level="4.4" data-path="4-classification-with-lda-qda-and-knn.html"><a href="4-classification-with-lda-qda-and-knn.html#quadratic-discriminent-analysis"><i class="fa fa-check"></i><b>4.4</b> Quadratic Discriminent Analysis</a></li>
<li class="chapter" data-level="4.5" data-path="4-classification-with-lda-qda-and-knn.html"><a href="4-classification-with-lda-qda-and-knn.html#examples"><i class="fa fa-check"></i><b>4.5</b> Examples</a><ul>
<li class="chapter" data-level="4.5.1" data-path="4-classification-with-lda-qda-and-knn.html"><a href="4-classification-with-lda-qda-and-knn.html#iris-data"><i class="fa fa-check"></i><b>4.5.1</b> Iris Data</a></li>
<li class="chapter" data-level="4.5.2" data-path="4-classification-with-lda-qda-and-knn.html"><a href="4-classification-with-lda-qda-and-knn.html#detecting-blood-doping"><i class="fa fa-check"></i><b>4.5.2</b> Detecting Blood Doping</a></li>
<li class="chapter" data-level="4.5.3" data-path="4-classification-with-lda-qda-and-knn.html"><a href="4-classification-with-lda-qda-and-knn.html#d-example"><i class="fa fa-check"></i><b>4.5.3</b> 2-d Example</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4-classification-with-lda-qda-and-knn.html"><a href="4-classification-with-lda-qda-and-knn.html#exercises-3"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-resampling-methods.html"><a href="5-resampling-methods.html"><i class="fa fa-check"></i><b>5</b> Resampling Methods</a><ul>
<li class="chapter" data-level="5.1" data-path="5-resampling-methods.html"><a href="5-resampling-methods.html#cross-validation"><i class="fa fa-check"></i><b>5.1</b> Cross-validation</a><ul>
<li class="chapter" data-level="5.1.1" data-path="5-resampling-methods.html"><a href="5-resampling-methods.html#validation-sets-approach"><i class="fa fa-check"></i><b>5.1.1</b> Validation Sets Approach</a></li>
<li class="chapter" data-level="5.1.2" data-path="5-resampling-methods.html"><a href="5-resampling-methods.html#leave-one-out-cross-validation-loocv."><i class="fa fa-check"></i><b>5.1.2</b> Leave one out Cross Validation (LOOCV).</a></li>
<li class="chapter" data-level="5.1.3" data-path="5-resampling-methods.html"><a href="5-resampling-methods.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>5.1.3</b> K-fold cross validation</a></li>
<li class="chapter" data-level="5.1.4" data-path="5-resampling-methods.html"><a href="5-resampling-methods.html#repeated-k-fold-cross-validation"><i class="fa fa-check"></i><b>5.1.4</b> Repeated K-fold cross validation</a></li>
<li class="chapter" data-level="5.1.5" data-path="5-resampling-methods.html"><a href="5-resampling-methods.html#using-cross-validation-to-select-a-tuning-parameter"><i class="fa fa-check"></i><b>5.1.5</b> Using cross validation to select a tuning parameter</a></li>
<li class="chapter" data-level="5.1.6" data-path="5-resampling-methods.html"><a href="5-resampling-methods.html#comparing-two-analysis-techniques"><i class="fa fa-check"></i><b>5.1.6</b> Comparing two analysis techniques</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="5-resampling-methods.html"><a href="5-resampling-methods.html#bootstrapping"><i class="fa fa-check"></i><b>5.2</b> Bootstrapping</a><ul>
<li class="chapter" data-level="5.2.1" data-path="5-resampling-methods.html"><a href="5-resampling-methods.html#observational-studies-vs-designed-experiments"><i class="fa fa-check"></i><b>5.2.1</b> Observational Studies vs Designed Experiments</a></li>
<li class="chapter" data-level="5.2.2" data-path="5-resampling-methods.html"><a href="5-resampling-methods.html#confidence-interval-types"><i class="fa fa-check"></i><b>5.2.2</b> Confidence Interval Types</a></li>
<li class="chapter" data-level="5.2.3" data-path="5-resampling-methods.html"><a href="5-resampling-methods.html#using-carboot-function"><i class="fa fa-check"></i><b>5.2.3</b> Using <code>car::Boot()</code> function</a></li>
<li class="chapter" data-level="5.2.4" data-path="5-resampling-methods.html"><a href="5-resampling-methods.html#using-the-boot-package"><i class="fa fa-check"></i><b>5.2.4</b> Using the <code>boot</code> package</a></li>
<li class="chapter" data-level="5.2.5" data-path="5-resampling-methods.html"><a href="5-resampling-methods.html#including-blockingstratifying-variables"><i class="fa fa-check"></i><b>5.2.5</b> Including Blocking/Stratifying Variables</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5-resampling-methods.html"><a href="5-resampling-methods.html#exercises-4"><i class="fa fa-check"></i><b>5.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-model-selection-and-regularization.html"><a href="6-model-selection-and-regularization.html"><i class="fa fa-check"></i><b>6</b> Model Selection and Regularization</a><ul>
<li class="chapter" data-level="6.1" data-path="6-model-selection-and-regularization.html"><a href="6-model-selection-and-regularization.html#stepwise-selection-using-aic"><i class="fa fa-check"></i><b>6.1</b> Stepwise selection using AIC</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-model-selection-and-regularization.html"><a href="6-model-selection-and-regularization.html#adjusted-r-sq"><i class="fa fa-check"></i><b>6.1.1</b> Adjusted <code>R-sq</code></a></li>
<li class="chapter" data-level="6.1.2" data-path="6-model-selection-and-regularization.html"><a href="6-model-selection-and-regularization.html#example"><i class="fa fa-check"></i><b>6.1.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-model-selection-and-regularization.html"><a href="6-model-selection-and-regularization.html#model-regularization-via-lasso-and-ridge-regression"><i class="fa fa-check"></i><b>6.2</b> Model Regularization via LASSO and Ridge Regression</a><ul>
<li class="chapter" data-level="6.2.1" data-path="6-model-selection-and-regularization.html"><a href="6-model-selection-and-regularization.html#regression"><i class="fa fa-check"></i><b>6.2.1</b> Regression</a></li>
<li class="chapter" data-level="6.2.2" data-path="6-model-selection-and-regularization.html"><a href="6-model-selection-and-regularization.html#classification"><i class="fa fa-check"></i><b>6.2.2</b> Classification</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-model-selection-and-regularization.html"><a href="6-model-selection-and-regularization.html#exercises-5"><i class="fa fa-check"></i><b>6.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-beyond-linearity.html"><a href="7-beyond-linearity.html"><i class="fa fa-check"></i><b>7</b> Beyond Linearity</a><ul>
<li class="chapter" data-level="7.1" data-path="7-beyond-linearity.html"><a href="7-beyond-linearity.html#locally-weighted-scatterplot-smoothing-loess"><i class="fa fa-check"></i><b>7.1</b> Locally Weighted Scatterplot Smoothing (LOESS)</a></li>
<li class="chapter" data-level="7.2" data-path="7-beyond-linearity.html"><a href="7-beyond-linearity.html#piecewise-linear"><i class="fa fa-check"></i><b>7.2</b> Piecewise linear</a></li>
<li class="chapter" data-level="7.3" data-path="7-beyond-linearity.html"><a href="7-beyond-linearity.html#smoothing-splines"><i class="fa fa-check"></i><b>7.3</b> Smoothing Splines</a></li>
<li class="chapter" data-level="7.4" data-path="7-beyond-linearity.html"><a href="7-beyond-linearity.html#gams"><i class="fa fa-check"></i><b>7.4</b> GAMS</a></li>
<li class="chapter" data-level="7.5" data-path="7-beyond-linearity.html"><a href="7-beyond-linearity.html#exercises-6"><i class="fa fa-check"></i><b>7.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-tree-based-algorithms.html"><a href="8-tree-based-algorithms.html"><i class="fa fa-check"></i><b>8</b> Tree Based Algorithms</a><ul>
<li class="chapter" data-level="8.1" data-path="8-tree-based-algorithms.html"><a href="8-tree-based-algorithms.html#regression-trees"><i class="fa fa-check"></i><b>8.1</b> Regression trees</a></li>
<li class="chapter" data-level="8.2" data-path="8-tree-based-algorithms.html"><a href="8-tree-based-algorithms.html#classification-trees"><i class="fa fa-check"></i><b>8.2</b> Classification trees</a></li>
<li class="chapter" data-level="8.3" data-path="8-tree-based-algorithms.html"><a href="8-tree-based-algorithms.html#bagging"><i class="fa fa-check"></i><b>8.3</b> Bagging</a></li>
<li class="chapter" data-level="8.4" data-path="8-tree-based-algorithms.html"><a href="8-tree-based-algorithms.html#random-forests-where-we-select-a-different-number-of-predictors"><i class="fa fa-check"></i><b>8.4</b> Random Forests where we select a different number of predictors</a></li>
<li class="chapter" data-level="8.5" data-path="8-tree-based-algorithms.html"><a href="8-tree-based-algorithms.html#boosting"><i class="fa fa-check"></i><b>8.5</b> Boosting</a></li>
<li class="chapter" data-level="8.6" data-path="8-tree-based-algorithms.html"><a href="8-tree-based-algorithms.html#exercises-7"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STA 578 - Statistical Computing Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="tree-based-algorithms" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Tree Based Algorithms</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(dplyr)</code></pre></div>
<pre><code>## 
## Attaching package: &#39;dplyr&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(ISLR)
<span class="kw">library</span>(tree)</code></pre></div>
<div id="regression-trees" class="section level2">
<h2><span class="header-section-number">8.1</span> Regression trees</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(fueleconomy)
<span class="kw">data</span>(mpg)
<span class="kw">str</span>(mpg)</code></pre></div>
<pre><code>## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:    234 obs. of  11 variables:
##  $ manufacturer: chr  &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; ...
##  $ model       : chr  &quot;a4&quot; &quot;a4&quot; &quot;a4&quot; &quot;a4&quot; ...
##  $ displ       : num  1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ...
##  $ year        : int  1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ...
##  $ cyl         : int  4 4 4 4 6 6 6 4 4 4 ...
##  $ trans       : chr  &quot;auto(l5)&quot; &quot;manual(m5)&quot; &quot;manual(m6)&quot; &quot;auto(av)&quot; ...
##  $ drv         : chr  &quot;f&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; ...
##  $ cty         : int  18 21 20 21 16 18 18 18 16 20 ...
##  $ hwy         : int  29 29 31 30 26 26 27 26 25 28 ...
##  $ fl          : chr  &quot;p&quot; &quot;p&quot; &quot;p&quot; &quot;p&quot; ...
##  $ class       : chr  &quot;compact&quot; &quot;compact&quot; &quot;compact&quot; &quot;compact&quot; ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">t &lt;-<span class="st"> </span><span class="kw">tree</span>( cty <span class="op">~</span><span class="st"> </span>displ <span class="op">+</span><span class="st"> </span>year <span class="op">+</span><span class="st"> </span>cyl <span class="op">+</span><span class="st"> </span>trans <span class="op">+</span><span class="st"> </span>drv, <span class="dt">data=</span>mpg)</code></pre></div>
<pre><code>## Warning in tree(cty ~ displ + year + cyl + trans + drv, data = mpg): NAs
## introduced by coercion</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(t); <span class="kw">text</span>(t)</code></pre></div>
<p><img src="Statistical_Computing_Notes_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Branch lengths are proportional to the decrease in impurity of the leaf.</p>
<p>We can force the branch lengths to be the same on the graph.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(t, <span class="dt">type=</span><span class="st">&#39;uniform&#39;</span>); <span class="kw">text</span>(t)</code></pre></div>
<p><img src="Statistical_Computing_Notes_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Another example… prostate cancer data</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(faraway)
t &lt;-<span class="st"> </span><span class="kw">tree</span> ( lpsa <span class="op">~</span><span class="st"> </span>., <span class="dt">data=</span>prostate )
<span class="kw">plot</span>(t); <span class="kw">text</span>(t)</code></pre></div>
<p><img src="Statistical_Computing_Notes_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Another exmple… CPU performance</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(cpus, <span class="dt">package=</span><span class="st">&quot;MASS&quot;</span>)
cpus.ltr &lt;-<span class="st"> </span><span class="kw">tree</span>(<span class="kw">log10</span>(perf) <span class="op">~</span><span class="st"> </span>syct<span class="op">+</span>mmin<span class="op">+</span>mmax<span class="op">+</span>cach<span class="op">+</span>chmin<span class="op">+</span>chmax, cpus)
cpus.ltr</code></pre></div>
<pre><code>## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 209 43.12000 1.753  
##    2) cach &lt; 27 143 11.79000 1.525  
##      4) mmax &lt; 6100 78  3.89400 1.375  
##        8) mmax &lt; 1750 12  0.78430 1.089 *
##        9) mmax &gt; 1750 66  1.94900 1.427 *
##      5) mmax &gt; 6100 65  4.04500 1.704  
##       10) syct &lt; 360 58  2.50100 1.756  
##         20) chmin &lt; 5.5 46  1.22600 1.699 *
##         21) chmin &gt; 5.5 12  0.55070 1.974 *
##       11) syct &gt; 360 7  0.12910 1.280 *
##    3) cach &gt; 27 66  7.64300 2.249  
##      6) mmax &lt; 28000 41  2.34100 2.062  
##       12) cach &lt; 96.5 34  1.59200 2.008  
##         24) mmax &lt; 11240 14  0.42460 1.827 *
##         25) mmax &gt; 11240 20  0.38340 2.135 *
##       13) cach &gt; 96.5 7  0.17170 2.324 *
##      7) mmax &gt; 28000 25  1.52300 2.555  
##       14) cach &lt; 56 7  0.06929 2.268 *
##       15) cach &gt; 56 18  0.65350 2.667 *</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(cpus.ltr)</code></pre></div>
<pre><code>## 
## Regression tree:
## tree(formula = log10(perf) ~ syct + mmin + mmax + cach + chmin + 
##     chmax, data = cpus)
## Variables actually used in tree construction:
## [1] &quot;cach&quot;  &quot;mmax&quot;  &quot;syct&quot;  &quot;chmin&quot;
## Number of terminal nodes:  10 
## Residual mean deviance:  0.03187 = 6.342 / 199 
## Distribution of residuals:
##       Min.    1st Qu.     Median       Mean    3rd Qu.       Max. 
## -0.4945000 -0.1191000  0.0003571  0.0000000  0.1141000  0.4680000</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(cpus.ltr);  <span class="kw">text</span>(cpus.ltr)</code></pre></div>
<p><img src="Statistical_Computing_Notes_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Build a much bigger tree</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># mindev = within node deviance must be larger than this (relative to root node deviance)</span>
<span class="co">#  to be eligible to be split.  So smaller mindev -&gt; allowed to split nodes even </span>
<span class="co">#  when the decrease in deviance is small.</span>
cpus.ltr &lt;-<span class="st"> </span><span class="kw">tree</span>(<span class="kw">log10</span>(perf) <span class="op">~</span><span class="st"> </span>syct<span class="op">+</span>mmin<span class="op">+</span>mmax<span class="op">+</span>cach<span class="op">+</span>chmin<span class="op">+</span>chmax, cpus,
                 <span class="dt">mindev=</span>.<span class="dv">001</span>)
<span class="kw">plot</span>(cpus.ltr)</code></pre></div>
<p><img src="Statistical_Computing_Notes_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">t.small &lt;-<span class="st"> </span><span class="kw">prune.tree</span>(cpus.ltr, <span class="dt">best=</span><span class="dv">5</span>)
<span class="kw">plot</span>(t.small); <span class="kw">text</span>(t.small)</code></pre></div>
<p><img src="Statistical_Computing_Notes_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">t &lt;-<span class="st"> </span><span class="kw">tree</span>( price <span class="op">~</span><span class="st"> </span>carat<span class="op">+</span>cut<span class="op">+</span>color<span class="op">+</span>clarity, <span class="dt">data=</span>diamonds )
<span class="kw">plot</span>(t)
<span class="kw">text</span>(t)</code></pre></div>
<p><img src="Statistical_Computing_Notes_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">t.small &lt;-<span class="st"> </span><span class="kw">prune.tree</span>(t, <span class="dt">best=</span><span class="dv">5</span>)
<span class="kw">plot</span>(t.small)
<span class="kw">text</span>(t.small)</code></pre></div>
<p><img src="Statistical_Computing_Notes_files/figure-html/unnamed-chunk-9-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">t.small &lt;-<span class="st"> </span><span class="kw">prune.tree</span>(t, <span class="dt">best=</span><span class="dv">6</span>)
<span class="kw">plot</span>(t.small) 
<span class="kw">text</span>(t.small)</code></pre></div>
<p><img src="Statistical_Computing_Notes_files/figure-html/unnamed-chunk-9-3.png" width="672" /></p>
<p>How large a tree should we use? Cross-Validation!</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">5730</span>)
train &lt;-<span class="st"> </span>diamonds <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sample_frac</span>(.<span class="dv">5</span>)
test  &lt;-<span class="st"> </span><span class="kw">setdiff</span>(diamonds, train)

t &lt;-<span class="st"> </span><span class="kw">tree</span>( price <span class="op">~</span><span class="st"> </span>., <span class="dt">data=</span>train )
cv.result &lt;-<span class="st"> </span><span class="kw">cv.tree</span>(t, <span class="dt">K=</span><span class="dv">10</span>)
<span class="kw">plot</span>(cv.result)</code></pre></div>
<p><img src="Statistical_Computing_Notes_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">t.small &lt;-<span class="st"> </span><span class="kw">prune.tree</span>(t, <span class="dt">best=</span><span class="dv">4</span>)
<span class="kw">plot</span>(t.small)
<span class="kw">text</span>(t.small) </code></pre></div>
<p><img src="Statistical_Computing_Notes_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
</div>
<div id="classification-trees" class="section level2">
<h2><span class="header-section-number">8.2</span> Classification trees</h2>
<p>Classification trees work identically to regression trees, only with a different measure of node purity.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(Carseats)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    400 obs. of  11 variables:
##  $ Sales      : num  9.5 11.22 10.06 7.4 4.15 ...
##  $ CompPrice  : num  138 111 113 117 141 124 115 136 132 132 ...
##  $ Income     : num  73 48 35 100 64 113 105 81 110 113 ...
##  $ Advertising: num  11 16 10 4 3 13 0 15 0 0 ...
##  $ Population : num  276 260 269 466 340 501 45 425 108 131 ...
##  $ Price      : num  120 83 80 97 128 72 108 120 124 124 ...
##  $ ShelveLoc  : Factor w/ 3 levels &quot;Bad&quot;,&quot;Good&quot;,&quot;Medium&quot;: 1 2 3 3 1 1 3 2 3 3 ...
##  $ Age        : num  42 65 59 55 38 78 71 67 76 76 ...
##  $ Education  : num  17 10 12 14 13 16 15 10 10 17 ...
##  $ Urban      : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 1 2 2 1 1 ...
##  $ US         : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 1 2 1 2 1 2 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># make a categorical response out of Sales</span>
Carseats &lt;-<span class="st"> </span>Carseats <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>( <span class="dt">High =</span> <span class="kw">factor</span>(<span class="kw">ifelse</span>(Sales <span class="op">&gt;=</span><span class="st"> </span><span class="dv">8</span>, <span class="st">&#39;High&#39;</span>,<span class="st">&#39;Low&#39;</span>)))</code></pre></div>
<p>Now we fit the tree using exactly the same syntax as before</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">my.tree &lt;-<span class="st"> </span><span class="kw">tree</span>( High <span class="op">~</span><span class="st"> </span>., Carseats)
<span class="kw">plot</span>(my.tree)
<span class="kw">text</span>(my.tree)</code></pre></div>
<p><img src="Statistical_Computing_Notes_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>Silly us, there is clearly a best predictor in our data… Sales! The variable we used to create our categorical response variable.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">my.tree &lt;-<span class="st"> </span><span class="kw">tree</span>( High <span class="op">~</span><span class="st"> </span>. <span class="op">-</span><span class="st"> </span>Sales, Carseats)
<span class="kw">plot</span>(my.tree)
<span class="kw">text</span>(my.tree, <span class="dt">pretty=</span><span class="dv">0</span>)</code></pre></div>
<p><img src="Statistical_Computing_Notes_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(my.tree, <span class="dt">type=</span><span class="st">&#39;uniform&#39;</span>)
<span class="kw">text</span>(my.tree, <span class="dt">pretty=</span><span class="dv">0</span>)</code></pre></div>
<p><img src="Statistical_Computing_Notes_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>This is a very complex tree and is probably overfitting the data. Lets use cross-validation to pick the best size tree.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">345</span>)
<span class="co"># Start with the overfit tree</span>
my.tree &lt;-<span class="st"> </span><span class="kw">tree</span>( High <span class="op">~</span><span class="st"> </span>. <span class="op">-</span><span class="st"> </span>Sales, Carseats)

<span class="co"># then prune using 10 fold CV where we assess on the misclassification rate</span>
cv.tree &lt;-<span class="st"> </span><span class="kw">cv.tree</span>( my.tree, <span class="dt">FUN=</span>prune.misclass, <span class="dt">K=</span><span class="dv">10</span>)
cv.tree</code></pre></div>
<pre><code>## $size
##  [1] 27 26 24 22 19 17 14 12  7  6  5  3  2  1
## 
## $dev
##  [1] 104 105 105 101 101  97  99  91  97  99  94  98 117 165
## 
## $k
##  [1]      -Inf  0.000000  0.500000  1.000000  1.333333  1.500000  1.666667
##  [8]  2.500000  3.800000  4.000000  5.000000  7.500000 18.000000 47.000000
## 
## $method
## [1] &quot;misclass&quot;
## 
## attr(,&quot;class&quot;)
## [1] &quot;prune&quot;         &quot;tree.sequence&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cv.tree<span class="op">$</span>size[ <span class="kw">which.min</span>(cv.tree<span class="op">$</span>dev) ]</code></pre></div>
<pre><code>## [1] 12</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># So the best tree (according to CV) has 12 leaves.</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># What if we prune using the deviance aka the Gini measure?</span>
cv.tree &lt;-<span class="st"> </span><span class="kw">cv.tree</span>( my.tree )
cv.tree</code></pre></div>
<pre><code>## $size
##  [1] 27 26 25 24 23 22 21 20 19 17 16 14 12 11  9  8  7  6  4  3  2  1
## 
## $dev
##  [1] 826.9589 836.7638 836.0794 840.6605 840.2832 828.8401 828.8401
##  [8] 701.3998 685.5561 665.2928 637.3993 606.2272 594.3136 563.1075
## [15] 548.7791 513.5985 500.2839 487.9836 485.8734 493.5000 497.5779
## [22] 546.9277
## 
## $k
##  [1]      -Inf  5.487169  5.554986  5.883875  6.356830  6.770937  6.916241
##  [8]  8.707541  8.849556  9.632187  9.850997 10.464072 11.246703 11.739662
## [15] 11.948400 13.135354 14.313015 18.992527 21.060122 24.699537 34.298711
## [22] 60.567546
## 
## $method
## [1] &quot;deviance&quot;
## 
## attr(,&quot;class&quot;)
## [1] &quot;prune&quot;         &quot;tree.sequence&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cv.tree<span class="op">$</span>size[ <span class="kw">which.min</span>(cv.tree<span class="op">$</span>dev) ]</code></pre></div>
<pre><code>## [1] 4</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># The best here has 3 leaves.</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Prune based on deviance</span>
pruned.tree &lt;-<span class="st"> </span><span class="kw">prune.tree</span>( my.tree, <span class="dt">best=</span><span class="dv">12</span> )
<span class="kw">plot</span>(pruned.tree); 
<span class="kw">text</span>(pruned.tree, <span class="dt">pretty=</span><span class="dv">0</span>)</code></pre></div>
<p><img src="Statistical_Computing_Notes_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(pruned.tree)</code></pre></div>
<pre><code>## 
## Classification tree:
## snip.tree(tree = my.tree, nodes = c(30L, 5L, 119L, 6L, 56L, 235L, 
## 4L, 31L))
## Variables actually used in tree construction:
## [1] &quot;ShelveLoc&quot;   &quot;Price&quot;       &quot;Advertising&quot; &quot;CompPrice&quot;   &quot;Age&quot;        
## Number of terminal nodes:  12 
## Residual mean deviance:  0.7673 = 297.7 / 388 
## Misclassification error rate: 0.1625 = 65 / 400</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Prune based on misclassification</span>
pruned.tree &lt;-<span class="st"> </span><span class="kw">prune.tree</span>( my.tree, <span class="dt">best=</span><span class="dv">12</span>, <span class="dt">method=</span><span class="st">&#39;misclas&#39;</span> )
<span class="kw">plot</span>(pruned.tree); 
<span class="kw">text</span>(pruned.tree, <span class="dt">pretty=</span><span class="dv">0</span>)</code></pre></div>
<p><img src="Statistical_Computing_Notes_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(pruned.tree)</code></pre></div>
<pre><code>## 
## Classification tree:
## snip.tree(tree = my.tree, nodes = c(13L, 30L, 5L, 12L, 28L, 4L, 
## 59L, 31L))
## Variables actually used in tree construction:
## [1] &quot;ShelveLoc&quot;   &quot;Price&quot;       &quot;Income&quot;      &quot;Advertising&quot; &quot;CompPrice&quot;  
## [6] &quot;Age&quot;        
## Number of terminal nodes:  12 
## Residual mean deviance:  0.7832 = 303.9 / 388 
## Misclassification error rate: 0.14 = 56 / 400</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Prune based on misclassification</span>
pruned.tree2 &lt;-<span class="st"> </span><span class="kw">prune.tree</span>( my.tree, <span class="dt">best=</span><span class="dv">7</span>, <span class="dt">method=</span><span class="st">&#39;misclas&#39;</span> )
<span class="kw">plot</span>(pruned.tree2); 
<span class="kw">text</span>(pruned.tree, <span class="dt">pretty=</span><span class="dv">0</span>)</code></pre></div>
<p><img src="Statistical_Computing_Notes_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(pruned.tree2)</code></pre></div>
<pre><code>## 
## Classification tree:
## snip.tree(tree = my.tree, nodes = c(13L, 30L, 5L, 12L, 4L, 31L, 
## 14L))
## Variables actually used in tree construction:
## [1] &quot;ShelveLoc&quot;   &quot;Price&quot;       &quot;Income&quot;      &quot;Advertising&quot; &quot;Age&quot;        
## Number of terminal nodes:  7 
## Residual mean deviance:  0.9663 = 379.8 / 393 
## Misclassification error rate: 0.1875 = 75 / 400</code></pre>
</div>
<div id="bagging" class="section level2">
<h2><span class="header-section-number">8.3</span> Bagging</h2>
<p>What is the variability tree to tree? What happens if we have different data? What if I had a different 400 observations drawn from the population?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data.star &lt;-<span class="st"> </span>Carseats <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sample_frac</span>(<span class="dt">replace=</span><span class="ot">TRUE</span>)
my.tree &lt;-<span class="st"> </span><span class="kw">tree</span>(High <span class="op">~</span><span class="st"> </span>. <span class="op">-</span>Sales, <span class="dt">data=</span>data.star)
cv.tree &lt;-<span class="st"> </span><span class="kw">cv.tree</span>( my.tree, <span class="dt">FUN=</span>prune.misclass, <span class="dt">K=</span><span class="dv">10</span>)
size &lt;-<span class="st"> </span>cv.tree<span class="op">$</span>size[ <span class="kw">which.min</span>(cv.tree<span class="op">$</span>dev) ]
pruned.tree &lt;-<span class="st"> </span><span class="kw">prune.tree</span>( my.tree, <span class="dt">best=</span>size, <span class="dt">method=</span><span class="st">&#39;misclas&#39;</span> )
<span class="kw">plot</span>(pruned.tree)</code></pre></div>
<p><img src="Statistical_Computing_Notes_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#text(pruned.tree, pretty=0)</span></code></pre></div>
<p>These are highly variable! Lets us bagging to reduce variability…</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(randomForest)</code></pre></div>
<pre><code>## randomForest 4.6-12</code></pre>
<pre><code>## Type rfNews() to see new features/changes/bug fixes.</code></pre>
<pre><code>## 
## Attaching package: &#39;randomForest&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     margin</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     combine</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">bagged &lt;-<span class="st"> </span><span class="kw">randomForest</span>( High <span class="op">~</span><span class="st"> </span>. <span class="op">-</span><span class="st"> </span>Sales, <span class="dt">data=</span>Carseats, 
                        <span class="dt">mtry=</span><span class="dv">10</span>,        <span class="co"># Number of covariates to use in each tree</span>
                        <span class="dt">imporance=</span><span class="ot">TRUE</span>, <span class="co"># Assess the importance of each covariate</span>
                        <span class="dt">ntree =</span> <span class="dv">500</span>)    <span class="co"># number of trees to grow</span>
bagged</code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = High ~ . - Sales, data = Carseats, mtry = 10,      imporance = TRUE, ntree = 500) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 10
## 
##         OOB estimate of  error rate: 19%
## Confusion matrix:
##      High Low class.error
## High  116  48   0.2926829
## Low    28 208   0.1186441</code></pre>
<p>What are the most important predictors?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">importance</span>(bagged)</code></pre></div>
<pre><code>##             MeanDecreaseGini
## CompPrice         24.3272828
## Income            18.2571468
## Advertising       21.8537267
## Population        11.3755784
## Price             51.4371819
## ShelveLoc         35.9214757
## Age               20.1961257
## Education          6.6399499
## Urban              0.7463221
## US                 2.4785000</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">varImpPlot</span>(bagged)</code></pre></div>
<p><img src="Statistical_Computing_Notes_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
</div>
<div id="random-forests-where-we-select-a-different-number-of-predictors" class="section level2">
<h2><span class="header-section-number">8.4</span> Random Forests where we select a different number of predictors</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p &lt;-<span class="st"> </span><span class="dv">10</span>
r.forest &lt;-<span class="st"> </span><span class="kw">randomForest</span>( High <span class="op">~</span><span class="st"> </span>. <span class="op">-</span><span class="st"> </span>Sales, <span class="dt">data=</span>Carseats, 
                          <span class="dt">mtry=</span>p<span class="op">/</span><span class="dv">2</span>,        <span class="co"># Number of covariates to use in each tree</span>
                          <span class="dt">imporance=</span><span class="ot">TRUE</span>,  <span class="co"># Assess the importance of each covariate</span>
                          <span class="dt">ntree =</span> <span class="dv">500</span>)     <span class="co"># number of trees to grow</span>
r.forest</code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = High ~ . - Sales, data = Carseats, mtry = p/2,      imporance = TRUE, ntree = 500) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 5
## 
##         OOB estimate of  error rate: 20%
## Confusion matrix:
##      High Low class.error
## High  109  55   0.3353659
## Low    25 211   0.1059322</code></pre>
</div>
<div id="boosting" class="section level2">
<h2><span class="header-section-number">8.5</span> Boosting</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(gbm) <span class="co"># generalized boost models</span></code></pre></div>
<pre><code>## Loading required package: survival</code></pre>
<pre><code>## 
## Attaching package: &#39;survival&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:faraway&#39;:
## 
##     rats</code></pre>
<pre><code>## Loading required package: lattice</code></pre>
<pre><code>## 
## Attaching package: &#39;lattice&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:faraway&#39;:
## 
##     melanoma</code></pre>
<pre><code>## Loading required package: splines</code></pre>
<pre><code>## Loading required package: parallel</code></pre>
<pre><code>## Loaded gbm 2.1.3</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">boost &lt;-<span class="st"> </span><span class="kw">gbm</span>(High <span class="op">~</span><span class="st"> </span>. <span class="op">-</span><span class="st"> </span>Sales, 
    <span class="dt">data=</span>Carseats <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">High =</span> <span class="kw">as.integer</span>(High)<span class="op">-</span><span class="dv">1</span>),  <span class="co"># wants {0,1}</span>
    <span class="dt">distribution =</span> <span class="st">&#39;bernoulli&#39;</span>,  <span class="co"># use gaussian for regression trees</span>
    <span class="dt">interaction.depth =</span> <span class="dv">2</span>,  <span class="dt">n.trees =</span> <span class="dv">2000</span>, <span class="dt">shrinkage=</span>.<span class="dv">01</span> )
<span class="kw">summary</span>(boost)</code></pre></div>
<p><img src="Statistical_Computing_Notes_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<pre><code>##                     var    rel.inf
## Price             Price 28.1086622
## ShelveLoc     ShelveLoc 21.1310250
## CompPrice     CompPrice 14.8511548
## Advertising Advertising 12.9798309
## Age                 Age  9.8908060
## Income           Income  8.4823546
## Population   Population  2.4097468
## Education     Education  1.5514692
## Urban             Urban  0.3554677
## US                   US  0.2394828</code></pre>
<p>Ok, so we have a bunch of techniques so it will pay to investigate how well they predict.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">results &lt;-<span class="st"> </span><span class="ot">NULL</span>

<span class="cf">for</span>( i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">200</span>){
  temp  &lt;-<span class="st"> </span>Carseats <span class="op">%&gt;%</span><span class="st"> </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="op">-</span>Sales)
  test  &lt;-<span class="st"> </span>temp <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sample_frac</span>(.<span class="dv">5</span>)
  train &lt;-<span class="st"> </span><span class="kw">setdiff</span>(temp, test)

  my.tree &lt;-<span class="st"> </span><span class="kw">tree</span>( High <span class="op">~</span><span class="st"> </span>., <span class="dt">data=</span>train)
  cv.tree &lt;-<span class="st"> </span><span class="kw">cv.tree</span>( my.tree, <span class="dt">FUN=</span>prune.misclass, <span class="dt">K=</span><span class="dv">10</span>)
  num.leaves &lt;-<span class="st"> </span>cv.tree<span class="op">$</span>size[<span class="kw">which.min</span>(cv.tree<span class="op">$</span>dev)]
  pruned.tree &lt;-<span class="st"> </span><span class="kw">prune.tree</span>( my.tree, <span class="dt">best=</span>num.leaves, <span class="dt">method=</span><span class="st">&#39;misclas&#39;</span> )
  yhat &lt;-<span class="st"> </span><span class="kw">predict</span>(pruned.tree, <span class="dt">newdata=</span>test, <span class="dt">type=</span><span class="st">&#39;class&#39;</span>)
  results &lt;-<span class="st"> </span><span class="kw">rbind</span>(results, <span class="kw">data.frame</span>(<span class="dt">misclass=</span><span class="kw">mean</span>( yhat <span class="op">!=</span><span class="st"> </span>test<span class="op">$</span>High ),
                                       <span class="dt">type=</span><span class="st">&#39;CV-Prune&#39;</span>))

  bagged &lt;-<span class="st"> </span><span class="kw">randomForest</span>( High <span class="op">~</span><span class="st"> </span>., <span class="dt">data=</span>train, <span class="dt">mtry=</span>p)
  yhat   &lt;-<span class="st"> </span><span class="kw">predict</span>(bagged, <span class="dt">newdata=</span>test, <span class="dt">type=</span><span class="st">&#39;class&#39;</span>)
  results &lt;-<span class="st"> </span><span class="kw">rbind</span>(results, <span class="kw">data.frame</span>(<span class="dt">misclass=</span><span class="kw">mean</span>( yhat <span class="op">!=</span><span class="st"> </span>test<span class="op">$</span>High ),
                                       <span class="dt">type=</span><span class="st">&#39;Bagged&#39;</span>))

  RF     &lt;-<span class="st"> </span><span class="kw">randomForest</span>( High <span class="op">~</span><span class="st"> </span>., <span class="dt">data=</span>train, <span class="dt">mtry=</span>p<span class="op">/</span><span class="dv">2</span>)
  yhat   &lt;-<span class="st"> </span><span class="kw">predict</span>(RF, <span class="dt">newdata=</span>test, <span class="dt">type=</span><span class="st">&#39;class&#39;</span>)
  results &lt;-<span class="st"> </span><span class="kw">rbind</span>(results, <span class="kw">data.frame</span>(<span class="dt">misclass=</span><span class="kw">mean</span>( yhat <span class="op">!=</span><span class="st"> </span>test<span class="op">$</span>High ),
                                       <span class="dt">type=</span><span class="st">&#39;RF - p/2&#39;</span>))

  RF     &lt;-<span class="st"> </span><span class="kw">randomForest</span>( High <span class="op">~</span><span class="st"> </span>., <span class="dt">data=</span>train, <span class="dt">mtry=</span><span class="kw">sqrt</span>(p))
  yhat   &lt;-<span class="st"> </span><span class="kw">predict</span>(RF, <span class="dt">newdata=</span>test, <span class="dt">type=</span><span class="st">&#39;class&#39;</span>)
  results &lt;-<span class="st"> </span><span class="kw">rbind</span>(results, <span class="kw">data.frame</span>(<span class="dt">misclass=</span><span class="kw">mean</span>( yhat <span class="op">!=</span><span class="st"> </span>test<span class="op">$</span>High ),
                                       <span class="dt">type=</span><span class="st">&#39;RF - sqrt(p)&#39;</span>))
  
  boost &lt;-<span class="st"> </span><span class="kw">gbm</span>(High <span class="op">~</span><span class="st"> </span>., 
    <span class="dt">data=</span>train <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">High =</span> <span class="kw">as.integer</span>(High)<span class="op">-</span><span class="dv">1</span>),  <span class="co"># wants {0,1}</span>
    <span class="dt">distribution =</span> <span class="st">&#39;bernoulli&#39;</span>,  <span class="co"># use gaussian for regression trees</span>
    <span class="dt">interaction.depth =</span> <span class="dv">2</span>,  <span class="dt">n.trees =</span> <span class="dv">2000</span>, <span class="dt">shrinkage=</span>.<span class="dv">01</span> )
  yhat &lt;-<span class="st"> </span><span class="kw">predict</span>(boost, <span class="dt">newdata=</span>test, <span class="dt">n.trees=</span><span class="dv">2000</span>, <span class="dt">type=</span><span class="st">&#39;response&#39;</span>)
  results &lt;-<span class="st"> </span><span class="kw">rbind</span>(results, <span class="kw">data.frame</span>(<span class="dt">misclass=</span><span class="kw">mean</span>( <span class="kw">round</span>(yhat) <span class="op">!=</span><span class="st"> </span><span class="kw">as.integer</span>(test<span class="op">$</span>High)<span class="op">-</span><span class="dv">1</span> ),
                                       <span class="dt">type=</span><span class="st">&#39;Boosting&#39;</span>))
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(results, <span class="kw">aes</span>(<span class="dt">x=</span>misclass, <span class="dt">y=</span>..density..)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth=</span><span class="fl">0.02</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_grid</span>(type <span class="op">~</span><span class="st"> </span>.)</code></pre></div>
<p><img src="Statistical_Computing_Notes_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">results <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>( type ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise</span>( <span class="dt">mean.misclass =</span> <span class="kw">mean</span>(misclass),
             <span class="dt">sd.misclass   =</span> <span class="kw">sd</span>(misclass))</code></pre></div>
<pre><code>## # A tibble: 5 x 3
##           type mean.misclass sd.misclass
##         &lt;fctr&gt;         &lt;dbl&gt;       &lt;dbl&gt;
## 1     CV-Prune      0.273225  0.03420636
## 2       Bagged      0.205975  0.02880230
## 3     RF - p/2      0.199450  0.02815869
## 4 RF - sqrt(p)      0.197125  0.02690219
## 5     Boosting      0.157025  0.02265149</code></pre>
</div>
<div id="exercises-7" class="section level2">
<h2><span class="header-section-number">8.6</span> Exercises</h2>
<ol style="list-style-type: decimal">
<li><p>ISLR #8.1 Draw an example (of your own invention) of a partition of a two dimensional feature space that could result from recursive binary splitting. Your example should contain at least six regions. Draw a decision tree corresponding to this partition. Be sure to label all aspects of your figures, including the regions <span class="math inline">\(R_1, R_2, \dots\)</span>, the cutpoints <span class="math inline">\(t_1, t_2, \dots\)</span>, and so forth.</p></li>
<li><p>ISLR #8.3. Consider the Gini index, classification error, and cross-entropy in a simple classification setting with two classes. Create a single plot that displays each of these quantities as a function of <span class="math inline">\(\hat{p}_{m1}\)</span>. The <span class="math inline">\(x\)</span>-axis should display <span class="math inline">\(\hat{p}_{m1}\)</span>, ranging from <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span>, and the <span class="math inline">\(y\)</span>-axis should display the value of the Gini index, classification error, and entropy.</p></li>
<li>ISLR #8.4 This question relates to the plots in Figure 8.12.
<ol style="list-style-type: lower-alpha">
<li>Sketch the tree corresponding to the partition of the predictor space illustrated in the left-hand panel of Figure 8.12. The numbers inside the boxes indicate the mean of <span class="math inline">\(Y\)</span> within each region.</li>
<li>Create a diagram similar to the left-hand panel of Figure 8.12, using the tree illustrated in the right-hand panel of the same figure. You should divide up the predictor space into the correct regions, and indicate the mean for each region.</li>
</ol></li>
<li>ISLR #8.8 In the lab, a classification tree was applied to the <code>Carseats</code> data set after converting <code>Sales</code> into a qualitative response variable. Now we will seek to predict <code>Sales</code> using regression trees and related approaches, treating the response as a quantitative variable.
<ol style="list-style-type: lower-alpha">
<li><p>Split the data set into a training set and a test set.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">9736</span>)
train &lt;-<span class="st"> </span>Carseats <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sample_frac</span>(<span class="fl">0.5</span>)
test  &lt;-<span class="st"> </span><span class="kw">setdiff</span>(Carseats, train)</code></pre></div></li>
<li>Fit a regression tree to the training set. Plot the tree, and interpret the results. What test error rate do you obtain?</li>
<li>Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test error rate?</li>
<li>Use the bagging approach in order to analyze this data. What test error rate do you obtain? Use the <code>importance()</code> function to determine which variables are most important.</li>
<li>Use random forests to analyze this data. What test error rate do you obtain? Use the <code>importance()</code> function to determine which variables are most important. Describe the effect of <span class="math inline">\(m\)</span>, the number of variables considered at each split, on the error rate obtained.</li>
<li><p>Use boosting to analyze this data. What test error rate do you obtain? Describe the effect of <span class="math inline">\(d\)</span>, the number of splits per step. Also describe the effect of changing <span class="math inline">\(\lambda\)</span> from 0.001, 0.01, and 0.1.</p></li>
</ol></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="7-beyond-linearity.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dereksonderegger/578/raw/master/08_Trees.Rmd",
"text": "Edit"
},
"download": [["Statistical_Computing_Notes.pdf", "PDF"], ["Statistical_Computing_Notes.epub", "EPUB"]],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
