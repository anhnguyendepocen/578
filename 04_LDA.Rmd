# Linear and Quadratic Discriminant Analysis

```{r, echo=FALSE}
# Unattach any packages that happen to already be loaded. In general this is unecessary
# but is important for the creation of the book to not have package namespaces
# fighting unexpectedly.
pkgs = names(sessionInfo()$otherPkgs)
if( length(pkgs > 0)){
  pkgs = paste('package:', pkgs, sep = "")
  for( i in 1:length(pkgs)){
    detach(pkgs[i], character.only = TRUE, force=TRUE)
  }
}
```

```{r}
library(MASS)     # lda function
library(dplyr)    # data frame manipulations
library(ggplot2)  # plotting
```

In this document we'll walk through linear and quadratic discriminant analysis. In particular, we'll try to make a secondary example of code to work from rather than just the books code.

First, recall the iris data set is 150 observations that measure leaf and sepal characteristics for three different species of iris.  We'll use the leaf characteristics to try to produce a classification rule.

We'll first split this into a training data set and a test data set.
```{r}
set.seed( 8675309 )  # so that the same training set is chosen every time...
iris$Obs_ID <- 1:150 # there are a couple of identical rows, we don't like that

# random select 1/2 from each species to be the training set
train <- iris %>% group_by(Species) %>% sample_n( 25 )
test  <- setdiff(iris, train)
```


```{r, fig.height=3, fig.width=6}
ggplot(train, aes(x=Sepal.Length, color=Species)) +
  geom_density()
```


While these certainly aren't normal and it isn't clear that the equal variance amongst groups is accurate, there is nothing that prevents us from assuming so and just doing LDA.

```{r}
train %>% group_by(Species) %>% 
  summarize(xbar=mean(Sepal.Length),
            sd  =sd(Sepal.Length))
```

We'll predict setosa if x < (4.988 + 5.812)/2.
We'll predict virginica if x > (5.812 + 6.412)/2
We'll predict versicolor otherwise.

```{r}
# have R do this
model <- lda(Species ~ Sepal.Length, data=train)
test$yhat <- predict(model, newdata=test)$class
table( Truth=test$Species, Prediction=test$yhat )
```
This is a little hard to read, but of the 25 observations that we know are setosa,
5 of them have been misclassified as versicolor.  Likewise, of the 25 observations
that are virginica, 22 are correctly identified as virginica while 3 are misclassified as 
versicolor.

Next, we will relax the assumption that the distributions have equal variance. 
```{r}
# have R do this
model <- qda(Species ~ Sepal.Length, data=train)
test$yhat <- predict(model, newdata=test)$class
table( Truth=test$Species, Prediction=test$yhat )
```
This has improved our accuracy as 4 setosa observations that were previously misclassified 
are now correctly classified.

We now wish to assess how accurate our model is. That is, what percent of the test
observations do we correctly classify?
```{r}
mean( test$Species == test$yhat )
```

# Another example
```{r, echo=FALSE}
library(devtools)
install_github('dereksonderegger/dsData')
library(dsData)
```

```{r}
library(devtools)
install_github('dereksonderegger/dsData')  # repository of my data sets
library(dsData)
data("Hemocrit", package='dsData')
```

We now consider a case where the number of observations is
not the same between groups. Here we consider the case where we are interested
in using hemocrit levels to detect if a cyclist is cheating.
```{r, fig.height=3, fig.width=6}
#?Hemocrit
ggplot(Hemocrit, aes(x=hemocrit, y=status)) + geom_point()
```

What if I just naively assume that all professional cyclists are clean? How 
accurate is this prediction scheme?
```{r}
mean(  Hemocrit$status == 'Clean' )
```

In this case, I am pretty accurate because we correctly classify 95% of the cases!
Clearly we should be more intelligent. Lets use the LDA to fit a model that uses
hemocrit.
```{r}
model <- lda(status ~ hemocrit, data=Hemocrit)
Hemocrit$yhat <- predict(model)$class
table( Truth=Hemocrit$status, Predicted=Hemocrit$yhat)
```
So this method basically looks to see if the hemocrit level is greater than 
```{r}
(47.91 + 50.14)/2
```
and calls them a cheater. Can we choose something a bit more clever?  The predict.lda()
function also returns the posterior probabilities for each class.  In this case, it
chooses the category with the highest probability (for two classes than means whichever
is greater than 0.50). We can create a different rule that labels somebody a cheater only 
if the posterior probability is greater than 0.8 or whatever.

```{r, fig.height=3}
pred <- predict(model)
Hemocrit$yhat <- ifelse( pred$posterior[,2] <= .8, 'Clean', 'Cheat' )
mean(  Hemocrit$status == Hemocrit$yhat )
ggplot(Hemocrit, aes(x=hemocrit, y=status, color=yhat)) + geom_point()
```
Great, now we have no false-positives, but a number of folks are getting away with cheating.
But what if we back that up, how many false positives do we get... What we want is a graph
that compares my false-positive numbers to the true-positives.

```{r}
cut.offs <- seq(0, 1, length=1001)
results <- data.frame(cut.off=cut.offs, 
                      false.positive.rate=rep(NA,1001), 
                       true.positive.rate=rep(NA,1001))
for( i in 1:length(cut.offs) ){
  Hemocrit$yhat <- ifelse( pred$posterior[,2] <= cut.offs[i], 'Clean', 'Cheat' )
  # get the factor levels ordered the same way that Hemocrit$status has
  Hemocrit$yhat <- factor(Hemocrit$yhat, levels=c('Clean', 'Cheat')) 
  
  confusion <- table(Truth=Hemocrit$status, Prediction=Hemocrit$yhat)  
  results[i,'false.positive.rate'] <- confusion[1,2] / sum( confusion[1,])
  results[i, 'true.positive.rate'] <- confusion[2,2] / sum( confusion[2,])
}
```

```{r, fig.width=5, fig.height=3}
# make sure we don't wiggle due to repeated observations
results <- results %>% arrange(false.positive.rate, true.positive.rate)
ggplot(results, aes(x=false.positive.rate, y=true.positive.rate)) + 
  geom_line() +
  ggtitle( 'ROC' )
```